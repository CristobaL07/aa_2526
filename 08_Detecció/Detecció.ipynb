{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "id": "7c30a21814599e8e"
      },
      "cell_type": "markdown",
      "source": [
        "En aquesta prctica treballareu amb un model de detecci贸 dobjectes basat en PyTorch i la xarxa neuronal VGG16. L'objectiu es modificar el model per adaptar-lo a un problema de detecci贸 d'objectes espec铆fic utilitzant un conjunt de dades simples.\n",
        "\n",
        "![Exemple](08_Detecci贸/imgs/img.png)\n",
        "\n",
        "Emprarem un *dataset* de detecci贸 d'objectes que cont茅 imatges d'estrelles. Aquest conjunt de dades 茅s senzill i ideal per a practicar t猫cniques de detecci贸 d'objectes. El podeu trobar a Kaggle al seg眉ent [enlla莽](https://www.kaggle.com/datasets/kishanj/simple-object-detection). Per carregar aquest tipus de dataset haurem d'implementar una classe personalitzada que hereti de `torch.utils.data.Dataset`."
      ],
      "id": "7c30a21814599e8e"
    },
    {
      "metadata": {
        "id": "fa0e6dca86c52550"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 29,
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EstrellesDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (str): Path al fitxer CSV amb anotacions.\n",
        "            img_dir (str): Carpeta on hi ha les imatges.\n",
        "            transforms (callable, optional): Transformacions per aplicar a les imatges.\n",
        "        \"\"\"\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        # Retorna el nombre d'imatges al dataset\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Obt茅 la fila corresponent\n",
        "        row = self.annotations.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Extreu la bounding box i l'etiqueta\n",
        "        bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
        "        label = row['label']\n",
        "\n",
        "        # Si tens transformacions, les apliques\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, bbox\n"
      ],
      "id": "fa0e6dca86c52550"
    },
    {
      "metadata": {
        "id": "64bb33f8c3c2b2c2"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparaci贸 del model\n",
        "\n",
        "Comen莽arem carregant el model VGG16 preentrenat i adaptant-lo per a la detecci贸 d'objectes. Afegirem capes addicionals per predir les caixes delimitadores (bounding boxes) i les classes dels objectes."
      ],
      "id": "64bb33f8c3c2b2c2"
    },
    {
      "metadata": {
        "id": "aad962e20c6a21ac"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 30,
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# modelo VGG16 preentrenado (ya lo tienes en tu c贸digo)\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "backbone = vgg16.features  # las capas convolucionales\n",
        "\n",
        "class VGG16ObjectDetector(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=2):\n",
        "        \"\"\"\n",
        "        backbone: las features convolucionales (vgg16.features)\n",
        "        num_classes: n煤mero de clases (incluyendo background si aplica)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # Reutilizamos el avgpool de la VGG preentrenada\n",
        "        self.avgpool = vgg16.avgpool\n",
        "\n",
        "        # Reutilizamos las capas fully-connected de VGG excepto la 煤ltima (la que produce 1000)\n",
        "        # stas transforman el flatten de 25088 a un vector de 4096\n",
        "        # list(vgg16.classifier.children()) es:\n",
        "        # [Linear(25088,4096), ReLU, Dropout, Linear(4096,4096), ReLU, Dropout, Linear(4096,1000)]\n",
        "        # Quitamos la 煤ltima Linear para quedarnos con salida 4096\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Sequential(*list(vgg16.classifier.children())[:-1])  # output: 4096-dim\n",
        "\n",
        "        # Cabeza de clasificaci贸n: de 4096 -> num_classes (logits)\n",
        "        self.class_head = nn.Linear(4096, num_classes)\n",
        "\n",
        "        # Cabeza de regresi贸n de bounding box: de 4096 -> 4 (xmin, ymin, xmax, ymax)\n",
        "        self.bbox_head = nn.Linear(4096, 4)\n",
        "\n",
        "        # Opcional: inicializaci贸n peque帽a para las cabezas nuevas\n",
        "        nn.init.normal_(self.class_head.weight, mean=0.0, std=0.01)\n",
        "        nn.init.constant_(self.class_head.bias, 0.0)\n",
        "        nn.init.normal_(self.bbox_head.weight, mean=0.0, std=0.001)\n",
        "        nn.init.constant_(self.bbox_head.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor [B, 3, H, W]\n",
        "        Retorna:\n",
        "            class_logits: [B, num_classes]\n",
        "            bbox_preds:   [B, 4]  (en el mismo orden que usemos: xmin, ymin, xmax, ymax)\n",
        "        \"\"\"\n",
        "        # 1) backbone convolucional\n",
        "        x = self.backbone(x)          # e.g. [B, 512, H', W']\n",
        "        x = self.avgpool(x)           # [B, 512, 7, 7] -> depende del tama帽o de entrada\n",
        "        x = self.flatten(x)           # [B, 512*7*7 = 25088]\n",
        "\n",
        "        # 2) fully-connected (reutilizadas de VGG excepto la 煤ltima capa)\n",
        "        feat = self.fc(x)             # [B, 4096]\n",
        "\n",
        "        # 3) cabezas separadas\n",
        "        class_logits = self.class_head(feat)  # [B, num_classes]\n",
        "        bbox_preds = self.bbox_head(feat)     # [B, 4]\n",
        "\n",
        "        return class_logits, bbox_preds\n",
        "\n",
        "# Ejemplo de instanciaci贸n\n",
        "model = VGG16ObjectDetector(backbone=backbone, num_classes=2)\n"
      ],
      "id": "aad962e20c6a21ac"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Transformaciones como en el entrenamiento\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Cargar una imagen de prueba\n",
        "img_path = \"/content/sample_data/a (10).jpg\"   #  c谩mbiala por la ruta real\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "x = transform(image).unsqueeze(0)  # [1, 3, 224, 224]\n"
      ],
      "metadata": {
        "id": "iGshQQaPp_lG"
      },
      "id": "iGshQQaPp_lG",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    class_logits, bbox_preds = model(x)\n"
      ],
      "metadata": {
        "id": "4oRRTdAGqkP8"
      },
      "id": "4oRRTdAGqkP8",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener probabilidades de clase\n",
        "probs = torch.softmax(class_logits, dim=1)\n",
        "pred_class = torch.argmax(probs, dim=1).item()\n",
        "confidence = probs[0][pred_class].item()\n",
        "\n",
        "# Obtener bounding box\n",
        "bbox = bbox_preds[0].tolist()\n",
        "\n",
        "print(f\"Pred class: {pred_class}, prob: {confidence:.3f}\")\n",
        "print(f\"Pred bbox: {bbox}\")\n"
      ],
      "metadata": {
        "id": "pJj-euqWq6MO",
        "outputId": "8652384c-dc88-4fc1-941e-a69d1b3db77c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pJj-euqWq6MO",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred class: 0, prob: 0.682\n",
            "Pred bbox: [-0.13005539774894714, 0.04336685687303543, 0.017337141558527946, 0.020896198228001595]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}